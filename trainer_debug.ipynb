{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Trainer Debug Notebook\n",
        "Quick debug notebook for online function testing without reloading data and models repeatedly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Place %autoreload 2 at the top of your notebook to ensure it applies to all subsequent imports.\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "import torch\n",
        "import os\n",
        "import sys\n",
        "from trainer import Trainer\n",
        "from options import MonodepthOptions\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Initialize Options\n",
        "Edit parameters here as needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Options initialized:\n",
            "  Dataset: endovis\n",
            "  Batch size: 2\n",
            "  Frame IDs: [0, -1, 1]\n",
            "  Compute metrics: True\n"
          ]
        }
      ],
      "source": [
        "# Initialize options\n",
        "options = MonodepthOptions()\n",
        "\n",
        "# Define arguments - edit as needed\n",
        "args = [\n",
        "    '--num_epochs', '50000',\n",
        "    '--num_workers', '1',\n",
        "    '--batch_size', '2',\n",
        "    '--log_frequency', '10',\n",
        "    '--save_frequency', '100000',\n",
        "    '--of_samples',\n",
        "    '--of_samples_num', '50',\n",
        "    '--frame_ids', '0', '-1', '1',\n",
        "    '--dataset', 'endovis',\n",
        "    '--data_path', '/mnt/nct-zfs/TCO-All/SharedDatasets/SCARED_Images_Resized/',\n",
        "    '--log_dir', '/mnt/nct-zfs/TCO-Test/jinjingxu/exps/train/mvp3r/results/unisfm/iidsfm',\n",
        "    '--compute_metrics',\n",
        "    '--reproj_supervise_type', 'reprojection_color_warp',\n",
        "    '--seed', '42',\n",
        "]\n",
        "\n",
        "# Parse options (use parse_notebook for notebook args list)\n",
        "opts = options.parse_notebook(args)\n",
        "\n",
        "# Override specific options if needed (edit here for quick changes)\n",
        "# opts.batch_size = 2\n",
        "# opts.num_workers = 1\n",
        "# opts.of_samples_num = 50\n",
        "# opts.compute_metrics = True\n",
        "\n",
        "print(\"Options initialized:\")\n",
        "print(f\"  Dataset: {opts.dataset}\")\n",
        "print(f\"  Batch size: {opts.batch_size}\")\n",
        "print(f\"  Frame IDs: {opts.frame_ids}\")\n",
        "print(f\"  Compute metrics: {opts.compute_metrics}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/mnt/cluster/environments/jinjingxu/pkg/envs/cu12/lib/python3.11/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
            "  warnings.warn(\n",
            "/mnt/cluster/environments/jinjingxu/pkg/envs/cu12/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/mnt/cluster/environments/jinjingxu/pkg/envs/cu12/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model named:\n",
            "   2025-12-05-11-34-22\n",
            "Models and tensorboard events files are saved to:\n",
            "   /mnt/nct-zfs/TCO-Test/jinjingxu/exps/train/mvp3r/results/unisfm/iidsfm\n",
            "Training is using:\n",
            "   cuda\n",
            "Overfitting mode: using 50 samples\n",
            "Using split:\n",
            "   endovis\n",
            "There are 50 training items and 1705 validation items\n",
            "\n",
            "Trainer initialized successfully!\n",
            "  Device: cuda\n",
            "  Number of models: 7\n",
            "  Models: ['encoder', 'depth', 'decompose_encoder', 'decompose', 'adjust_net', 'pose_encoder', 'pose']\n"
          ]
        }
      ],
      "source": [
        "# Initialize trainer\n",
        "trainer = Trainer(opts)\n",
        "\n",
        "print(\"Trainer initialized successfully!\")\n",
        "print(f\"  Device: {trainer.device}\")\n",
        "print(f\"  Number of models: {len(trainer.models)}\")\n",
        "print(f\"  Models: {list(trainer.models.keys())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Trained Models (Optional)\n",
        "Load pretrained weights if available\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Models ready (no pretrained weights loaded)\n"
          ]
        }
      ],
      "source": [
        "# Uncomment and set path to load pretrained models\n",
        "# load_weights_folder = \"/path/to/weights/folder\"\n",
        "# opts.load_weights_folder = load_weights_folder\n",
        "# opts.models_to_load = [\"encoder\", \"depth\", \"pose_encoder\", \"pose\", \"decompose_encoder\", \"decompose\", \"adjust_net\"]\n",
        "# trainer.load_model()\n",
        "\n",
        "print(\"Models ready (no pretrained weights loaded)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Initialize Data Loaders\n",
        "Get iterators for training and validation data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training batches: 25\n",
            "Validation batches: 852\n",
            "Data loaders ready!\n"
          ]
        }
      ],
      "source": [
        "# Data loaders are already initialized in Trainer.__init__\n",
        "# Access them directly\n",
        "train_loader = trainer.train_loader\n",
        "val_loader = trainer.val_loader\n",
        "\n",
        "print(f\"Training batches: {len(train_loader)}\")\n",
        "print(f\"Validation batches: {len(val_loader)}\")\n",
        "\n",
        "# Get a sample batch for testing\n",
        "train_iter = iter(train_loader)\n",
        "val_iter = iter(val_loader)\n",
        "\n",
        "print(\"Data loaders ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Test Single Batch Processing\n",
        "Test forward pass on a single batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample batch keys: [('K', 0), ('inv_K', 0), ('K', 1), ('inv_K', 1), ('K', 2), ('inv_K', 2), ('K', 3), ('inv_K', 3), ('color', 0, 0), ('color', 0, 1), ('color', 0, 2), ('color', 0, 3), ('color', -1, 0), ('color', -1, 1), ('color', -1, 2), ('color', -1, 3), ('color', 1, 0), ('color', 1, 1), ('color', 1, 2), ('color', 1, 3), ('color_aug', 0, 0), ('color_aug', 0, 1), ('color_aug', 0, 2), ('color_aug', 0, 3), ('color_aug', -1, 0), ('color_aug', -1, 1), ('color_aug', -1, 2), ('color_aug', -1, 3), ('color_aug', 1, 0), ('color_aug', 1, 1), ('color_aug', 1, 2), ('color_aug', 1, 3), ('depth_gt', 0, 0)]\n",
            "  ('K', 0): shape=torch.Size([2, 4, 4]), dtype=torch.float32, device=cuda:0\n",
            "  ('inv_K', 0): shape=torch.Size([2, 4, 4]), dtype=torch.float32, device=cuda:0\n",
            "  ('K', 1): shape=torch.Size([2, 4, 4]), dtype=torch.float32, device=cuda:0\n",
            "  ('inv_K', 1): shape=torch.Size([2, 4, 4]), dtype=torch.float32, device=cuda:0\n",
            "  ('K', 2): shape=torch.Size([2, 4, 4]), dtype=torch.float32, device=cuda:0\n",
            "  ('inv_K', 2): shape=torch.Size([2, 4, 4]), dtype=torch.float32, device=cuda:0\n",
            "  ('K', 3): shape=torch.Size([2, 4, 4]), dtype=torch.float32, device=cuda:0\n",
            "  ('inv_K', 3): shape=torch.Size([2, 4, 4]), dtype=torch.float32, device=cuda:0\n",
            "  ('color', 0, 0): shape=torch.Size([2, 3, 256, 320]), dtype=torch.float32, device=cuda:0\n",
            "  ('color', 0, 1): shape=torch.Size([2, 3, 128, 160]), dtype=torch.float32, device=cuda:0\n",
            "  ('color', 0, 2): shape=torch.Size([2, 3, 64, 80]), dtype=torch.float32, device=cuda:0\n",
            "  ('color', 0, 3): shape=torch.Size([2, 3, 32, 40]), dtype=torch.float32, device=cuda:0\n",
            "  ('color', -1, 0): shape=torch.Size([2, 3, 256, 320]), dtype=torch.float32, device=cuda:0\n",
            "  ('color', -1, 1): shape=torch.Size([2, 3, 128, 160]), dtype=torch.float32, device=cuda:0\n",
            "  ('color', -1, 2): shape=torch.Size([2, 3, 64, 80]), dtype=torch.float32, device=cuda:0\n",
            "  ('color', -1, 3): shape=torch.Size([2, 3, 32, 40]), dtype=torch.float32, device=cuda:0\n",
            "  ('color', 1, 0): shape=torch.Size([2, 3, 256, 320]), dtype=torch.float32, device=cuda:0\n",
            "  ('color', 1, 1): shape=torch.Size([2, 3, 128, 160]), dtype=torch.float32, device=cuda:0\n",
            "  ('color', 1, 2): shape=torch.Size([2, 3, 64, 80]), dtype=torch.float32, device=cuda:0\n",
            "  ('color', 1, 3): shape=torch.Size([2, 3, 32, 40]), dtype=torch.float32, device=cuda:0\n",
            "  ('color_aug', 0, 0): shape=torch.Size([2, 3, 256, 320]), dtype=torch.float32, device=cuda:0\n",
            "  ('color_aug', 0, 1): shape=torch.Size([2, 3, 128, 160]), dtype=torch.float32, device=cuda:0\n",
            "  ('color_aug', 0, 2): shape=torch.Size([2, 3, 64, 80]), dtype=torch.float32, device=cuda:0\n",
            "  ('color_aug', 0, 3): shape=torch.Size([2, 3, 32, 40]), dtype=torch.float32, device=cuda:0\n",
            "  ('color_aug', -1, 0): shape=torch.Size([2, 3, 256, 320]), dtype=torch.float32, device=cuda:0\n",
            "  ('color_aug', -1, 1): shape=torch.Size([2, 3, 128, 160]), dtype=torch.float32, device=cuda:0\n",
            "  ('color_aug', -1, 2): shape=torch.Size([2, 3, 64, 80]), dtype=torch.float32, device=cuda:0\n",
            "  ('color_aug', -1, 3): shape=torch.Size([2, 3, 32, 40]), dtype=torch.float32, device=cuda:0\n",
            "  ('color_aug', 1, 0): shape=torch.Size([2, 3, 256, 320]), dtype=torch.float32, device=cuda:0\n",
            "  ('color_aug', 1, 1): shape=torch.Size([2, 3, 128, 160]), dtype=torch.float32, device=cuda:0\n",
            "  ('color_aug', 1, 2): shape=torch.Size([2, 3, 64, 80]), dtype=torch.float32, device=cuda:0\n",
            "  ('color_aug', 1, 3): shape=torch.Size([2, 3, 32, 40]), dtype=torch.float32, device=cuda:0\n",
            "  ('depth_gt', 0, 0): shape=torch.Size([2, 1, 1024, 1280]), dtype=torch.float32, device=cuda:0\n",
            "\n",
            "Output keys: [('disp', 0), ('axisangle', 0, -1), ('translation', 0, -1), ('cam_T_cam', 0, -1), ('axisangle', 0, 1), ('translation', 0, 1), ('cam_T_cam', 0, 1), ('reflectance', 0, 0), ('light', 0, 0), ('reprojection_color', 0, 0), ('reflectance', 0, -1), ('light', 0, -1), ('reprojection_color', 0, -1), ('reflectance', 0, 1), ('light', 0, 1), ('reprojection_color', 0, 1), ('depth', 0, 0), ('warp', 0, -1), ('reflectance_warp', 0, -1), ('light_warp', 0, -1), ('color_warp', 0, -1), ('valid_mask', 0, -1), ('warp_diff_color', 0, -1), ('transform', 0, -1), ('light_adjust_warp', 0, -1), ('reprojection_color_warp', 0, -1), ('warp', 0, 1), ('reflectance_warp', 0, 1), ('light_warp', 0, 1), ('color_warp', 0, 1), ('valid_mask', 0, 1), ('warp_diff_color', 0, 1), ('transform', 0, 1), ('light_adjust_warp', 0, 1), ('reprojection_color_warp', 0, 1)]\n",
            "\n",
            "Losses:\n",
            "  loss: 0.400739\n",
            "  loss_reconstruction: 0.339050\n",
            "  loss_reflec: 0.001099\n",
            "  loss_reprojection: 0.332640\n",
            "  loss_disp_smooth: 0.006853\n"
          ]
        }
      ],
      "source": [
        "# Get a sample batch\n",
        "trainer.set_train()\n",
        "sample_inputs = next(train_iter)\n",
        "sample_inputs = next(val_iter) # gt_depth exists\n",
        "\n",
        "# Move inputs to device (process_batch does this, but we do it explicitly here for clarity)\n",
        "for key, val in sample_inputs.items():\n",
        "    if isinstance(val, torch.Tensor):\n",
        "        sample_inputs[key] = val.to(trainer.device)\n",
        "\n",
        "print(\"Sample batch keys:\", list(sample_inputs.keys()))\n",
        "for key, val in sample_inputs.items():\n",
        "    if isinstance(val, torch.Tensor):\n",
        "        print(f\"  {key}: shape={val.shape}, dtype={val.dtype}, device={val.device}\")\n",
        "\n",
        "# Process batch\n",
        "with torch.no_grad():\n",
        "    outputs, losses = trainer.process_batch(sample_inputs)\n",
        "\n",
        "print(\"\\nOutput keys:\", list(outputs.keys()))\n",
        "print(\"\\nLosses:\")\n",
        "for key, val in losses.items():\n",
        "    if isinstance(val, torch.Tensor):\n",
        "        print(f\"  {key}: {val.item():.6f}\")\n",
        "    else:\n",
        "        print(f\"  {key}: {val}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test Validation with Metrics\n",
        "Test validation forward pass and metrics computation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation batch keys: [('K', 0), ('inv_K', 0), ('K', 1), ('inv_K', 1), ('K', 2), ('inv_K', 2), ('K', 3), ('inv_K', 3), ('color', 0, 0), ('color', 0, 1), ('color', 0, 2), ('color', 0, 3), ('color', -1, 0), ('color', -1, 1), ('color', -1, 2), ('color', -1, 3), ('color', 1, 0), ('color', 1, 1), ('color', 1, 2), ('color', 1, 3), ('color_aug', 0, 0), ('color_aug', 0, 1), ('color_aug', 0, 2), ('color_aug', 0, 3), ('color_aug', -1, 0), ('color_aug', -1, 1), ('color_aug', -1, 2), ('color_aug', -1, 3), ('color_aug', 1, 0), ('color_aug', 1, 1), ('color_aug', 1, 2), ('color_aug', 1, 3), ('depth_gt', 0, 0)]\n",
            "  depth_gt: shape=torch.Size([2, 1, 1024, 1280]), device=cuda:0\n",
            "\n",
            "Validation Losses:\n",
            "  loss: 0.394922\n",
            "  loss_reconstruction: 0.335299\n",
            "  loss_reflec: 0.000678\n",
            "  loss_reprojection: 0.327701\n",
            "  loss_disp_smooth: 0.002639\n",
            "\n",
            "Depth Metrics:\n",
            "  abs_rel: 0.232254\n",
            "  sq_rel: 3.867111\n",
            "  rmse: 14.407352\n",
            "  rmse_log: 0.283523\n",
            "  a1: 0.579472\n",
            "  a2: 0.838206\n",
            "  a3: 0.999674\n",
            "  median_scaling_ratio: 245.557007\n",
            "  median_scaling_std: 0.003271\n"
          ]
        }
      ],
      "source": [
        "# Get validation batch\n",
        "trainer.set_eval()\n",
        "try:\n",
        "    val_inputs = next(val_iter)\n",
        "except StopIteration:\n",
        "    val_iter = iter(val_loader)\n",
        "    val_inputs = next(val_iter)\n",
        "\n",
        "# Move inputs to device\n",
        "for key, val in val_inputs.items():\n",
        "    if isinstance(val, torch.Tensor):\n",
        "        val_inputs[key] = val.to(trainer.device)\n",
        "\n",
        "print(\"Validation batch keys:\", list(val_inputs.keys()))\n",
        "if (\"depth_gt\", 0, 0) in val_inputs:\n",
        "    print(f\"  depth_gt: shape={val_inputs[('depth_gt', 0, 0)].shape}, device={val_inputs[('depth_gt', 0, 0)].device}\")\n",
        "\n",
        "# Process validation batch\n",
        "with torch.no_grad():\n",
        "    outputs, losses = trainer.process_batch(val_inputs)\n",
        "    \n",
        "    # Compute metrics if enabled\n",
        "    metrics = {}\n",
        "    if opts.compute_metrics:\n",
        "        from utils.metrics import compute_depth_metrics\n",
        "        metrics = compute_depth_metrics(val_inputs, outputs)\n",
        "\n",
        "print(\"\\nValidation Losses:\")\n",
        "for key, val in losses.items():\n",
        "    if isinstance(val, torch.Tensor):\n",
        "        print(f\"  {key}: {val.item():.6f}\")\n",
        "\n",
        "if metrics:\n",
        "    print(\"\\nDepth Metrics:\")\n",
        "    for key, val in metrics.items():\n",
        "        print(f\"  {key}: {val:.6f}\")\n",
        "else:\n",
        "    print(\"\\nNo metrics computed (GT depth not available or compute_metrics=False)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Explicit Training Loop\n",
        "Manual training loop for debugging (no tensorboard logging)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch    0 | Loss: 0.400739 | Time: 1.579s\n",
            "  Sub-losses: reconstruction=0.339050, reprojection=0.332640, reflec=0.001099, disp_smooth=0.006853\n",
            "Saved image grid to output_grid.png\n",
            "Batch    2 | Loss: 0.384031 | Time: 0.268s\n",
            "  Sub-losses: reconstruction=0.326219, reprojection=0.318423, reflec=0.001525, disp_smooth=0.005830\n",
            "Saved image grid to output_grid.png\n",
            "Batch    4 | Loss: 0.371374 | Time: 0.196s\n",
            "  Sub-losses: reconstruction=0.315245, reprojection=0.307922, reflec=0.001755, disp_smooth=0.005165\n",
            "Saved image grid to output_grid.png\n",
            "\n",
            "Training loop completed!\n"
          ]
        }
      ],
      "source": [
        "# Training loop parameters (edit as needed)\n",
        "num_batches_to_train = 5  # Number of batches to process\n",
        "log_every_n_batches = 2   # Log every N batches\n",
        "\n",
        "trainer.set_train()\n",
        "train_iter = iter(train_loader)\n",
        "train_iter = iter(val_loader)\n",
        "\n",
        "trainer.step = 0\n",
        "\n",
        "for batch_idx in range(num_batches_to_train):\n",
        "    try:\n",
        "        inputs = next(train_iter)\n",
        "    except StopIteration:\n",
        "        train_iter = iter(train_loader)\n",
        "        inputs = next(train_iter)\n",
        "    \n",
        "    # Note: process_batch moves inputs to device internally, but we can do it explicitly here too\n",
        "    # for key, val in inputs.items():\n",
        "    #     if isinstance(val, torch.Tensor):\n",
        "    #         inputs[key] = val.to(trainer.device)\n",
        "    \n",
        "    before_op_time = time.time()\n",
        "    \n",
        "    # Forward pass (process_batch handles device movement)\n",
        "    outputs, losses = trainer.process_batch(inputs)\n",
        "    \n",
        "    # Backward pass\n",
        "    trainer.model_optimizer.zero_grad()\n",
        "    losses[\"loss\"].backward()\n",
        "    trainer.model_optimizer.step()\n",
        "    \n",
        "    duration = time.time() - before_op_time\n",
        "    \n",
        "    # Log periodically\n",
        "    if batch_idx % log_every_n_batches == 0:\n",
        "        print(f\"Batch {batch_idx:4d} | Loss: {losses['loss'].item():.6f} | Time: {duration:.3f}s\")\n",
        "        print(f\"  Sub-losses: reconstruction={losses.get('loss_reconstruction', 0):.6f}, \"\n",
        "              f\"reprojection={losses.get('loss_reprojection', 0):.6f}, \"\n",
        "              f\"reflec={losses.get('loss_reflec', 0):.6f}, \"\n",
        "              f\"disp_smooth={losses.get('loss_disp_smooth', 0):.6f}\")\n",
        "    \n",
        "        # optional: log to tensorboard\n",
        "        # trainer.log_time(batch_idx, duration, losses[\"loss\"].cpu().data)\n",
        "        # trainer.log(\"train\", inputs, outputs, losses, metrics=None)\n",
        "        # trainer.val()\n",
        "\n",
        "        # optional: obtain various image from outputs and save as one row of images of all image\n",
        "        from utils import img_gen\n",
        "        for key in outputs.keys():\n",
        "            outputs[key] = outputs[key].detach()\n",
        "        # compute the depth_err metrics    \n",
        "        from utils.metrics import compute_depth_metrics\n",
        "        metrics = compute_depth_metrics(inputs, outputs)\n",
        "        merged_dict = {**inputs, **outputs, **metrics}\n",
        "        img_gen(\n",
        "            merged_dict=merged_dict,\n",
        "            image_keys=[(\"color\", 0, 0), (\"disp\", 0), (\"depth\", 0, 0), (\"depth_gt\", 0, 0), \"depth_err\"],\n",
        "            save_path=\"output_grid.png\",\n",
        "            sample_idx=0\n",
        "        )\n",
        "\n",
        "    # trainer.step += 1\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nTraining loop completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Test Specific Functions\n",
        "Test individual components\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decompose outputs:\n",
            "  ('reflectance', 0, 0): shape=torch.Size([2, 3, 256, 320])\n",
            "  ('light', 0, 0): shape=torch.Size([2, 1, 256, 320])\n",
            "  ('reflectance', 0, -1): shape=torch.Size([2, 3, 256, 320])\n",
            "  ('light', 0, -1): shape=torch.Size([2, 1, 256, 320])\n",
            "  ('reflectance', 0, 1): shape=torch.Size([2, 3, 256, 320])\n",
            "  ('light', 0, 1): shape=torch.Size([2, 1, 256, 320])\n",
            "\n",
            "Depth stored: True\n"
          ]
        }
      ],
      "source": [
        "# Test decompose function\n",
        "trainer.set_train()\n",
        "sample_inputs = next(iter(train_loader))\n",
        "\n",
        "# Move inputs to device\n",
        "for key, val in sample_inputs.items():\n",
        "    if isinstance(val, torch.Tensor):\n",
        "        sample_inputs[key] = val.to(trainer.device)\n",
        "\n",
        "# Get basic outputs first\n",
        "features = trainer.models[\"encoder\"](sample_inputs[(\"color_aug\", 0, 0)])\n",
        "outputs = trainer.models[\"depth\"](features)\n",
        "outputs.update(trainer.predict_poses(sample_inputs))\n",
        "\n",
        "# Test decompose\n",
        "trainer.decompose(sample_inputs, outputs)\n",
        "\n",
        "print(\"Decompose outputs:\")\n",
        "for key in outputs.keys():\n",
        "    if \"reflectance\" in key or \"light\" in key or \"reprojection\" in key:\n",
        "        if isinstance(outputs[key], torch.Tensor):\n",
        "            print(f\"  {key}: shape={outputs[key].shape}\")\n",
        "\n",
        "print(\"\\nDepth stored:\", (\"depth\", 0, 0) in outputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Inspect Model Parameters\n",
        "Check model states and parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "encoder             : 11,689,512 params (11,689,512 trainable)\n",
            "depth               :  3,152,724 params ( 3,152,724 trainable)\n",
            "decompose_encoder   : 11,689,512 params (11,689,512 trainable)\n",
            "decompose           :  3,273,732 params ( 3,273,732 trainable)\n",
            "adjust_net          :     19,425 params (    19,425 trainable)\n",
            "pose_encoder        : 11,698,920 params (11,698,920 trainable)\n",
            "pose                :  1,314,572 params ( 1,314,572 trainable)\n",
            "\n",
            "Total parameters: 42,838,397\n",
            "Total trainable: 42,838,397\n"
          ]
        }
      ],
      "source": [
        "# Count parameters\n",
        "total_params = 0\n",
        "for model_name, model in trainer.models.items():\n",
        "    model_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total_params += model_params\n",
        "    print(f\"{model_name:20s}: {model_params:>10,} params ({trainable_params:>10,} trainable)\")\n",
        "\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "print(f\"Total trainable: {sum(p.numel() for p in trainer.parameters_to_train):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Quick Parameter Edits\n",
        "Edit common parameters without reinitializing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current settings:\n",
            "  Batch size: 2\n",
            "  Learning rate: 0.0001\n",
            "  Compute metrics: True\n",
            "  Reproj supervise type: reprojection_color_warp\n"
          ]
        }
      ],
      "source": [
        "# Quick parameter edits (edit as needed)\n",
        "# opts.batch_size = 4\n",
        "# opts.learning_rate = 1e-5\n",
        "# opts.compute_metrics = True\n",
        "# opts.reproj_supervise_type = 'color_warp'\n",
        "\n",
        "# Update optimizer if learning rate changed\n",
        "# import torch.optim as optim\n",
        "# trainer.model_optimizer = optim.Adam(trainer.parameters_to_train, opts.learning_rate)\n",
        "\n",
        "print(\"Current settings:\")\n",
        "print(f\"  Batch size: {opts.batch_size}\")\n",
        "print(f\"  Learning rate: {opts.learning_rate}\")\n",
        "print(f\"  Compute metrics: {opts.compute_metrics}\")\n",
        "print(f\"  Reproj supervise type: {opts.reproj_supervise_type}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
